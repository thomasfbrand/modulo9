{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_w3vBvrAijd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88g7E1NTCYUj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klw-GA1gD8b5",
        "outputId": "cb319a44-092f-40c1-9c4c-e41f5030598e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média de passos com política aleatória: 50.61\n",
            "Média de passos com política ε-gulosa: 51.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridworldEpsilonGreedy:\n",
        "    def __init__(self, size=8, discount_factor=0.99, theta=0.01, epsilon=0.1):\n",
        "        self.size = size\n",
        "        self.discount_factor = discount_factor\n",
        "        self.theta = theta\n",
        "        self.epsilon = epsilon\n",
        "        self.state_values = np.zeros((size, size))\n",
        "        self.policy = np.ones((size, size, 4)) / 4\n",
        "        self.mountains = [(2, 2), (3, 3), (4, 4)]\n",
        "        self.quicksand = [(5, 2), (6, 3)]  #\n",
        "        self.end = (7, 7)\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return state == self.end\n",
        "\n",
        "    def get_next_state_and_reward(self, state, action):\n",
        "        if self.is_terminal(state):\n",
        "            return state, 0\n",
        "        if state in self.quicksand:\n",
        "            return state, -100\n",
        "\n",
        "        next_state = {\n",
        "            'up': (max(state[0] - 1, 0), state[1]),\n",
        "            'down': (min(state[0] + 1, self.size - 1), state[1]),\n",
        "            'left': (state[0], max(state[1] - 1, 0)),\n",
        "            'right': (state[0], min(state[1] + 1, self.size - 1))\n",
        "        }.get(action, state)\n",
        "\n",
        "        if next_state in self.mountains or next_state in self.quicksand:\n",
        "            next_state = state\n",
        "\n",
        "        reward = -100 if next_state in self.quicksand else -1\n",
        "        return next_state, reward\n",
        "\n",
        "    def evaluate_policy(self):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for i in range(self.size):\n",
        "                for j in range(self.size):\n",
        "                    if self.is_terminal((i, j)):\n",
        "                        continue\n",
        "                    v_old = self.state_values[i, j]\n",
        "                    v_new = 0\n",
        "                    for action_prob, action in zip(self.policy[i, j], [\"up\", \"down\", \"left\", \"right\"]):\n",
        "                        next_state, reward = self.get_next_state_and_reward((i, j), action)\n",
        "                        v_new += action_prob * (reward + self.discount_factor * self.state_values[next_state])\n",
        "                    self.state_values[i, j] = v_new\n",
        "                    delta = max(delta, abs(v_old - v_new))\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "    def improve_policy(self):\n",
        "        policy_stable = True\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                if self.is_terminal((i, j)):\n",
        "                    continue\n",
        "                best_action_value = float('-inf')\n",
        "                best_action = None\n",
        "                for action_index, action in enumerate([\"up\", \"down\", \"left\", \"right\"]):\n",
        "                    next_state, reward = self.get_next_state_and_reward((i, j), action)\n",
        "                    action_value = reward + self.discount_factor * self.state_values[next_state]\n",
        "                    if action_value > best_action_value:\n",
        "                        best_action_value = action_value\n",
        "                        best_action = action_index\n",
        "                for action_index in range(4):\n",
        "                    if action_index == best_action:\n",
        "                        self.policy[i, j, action_index] = 1 - self.epsilon + (self.epsilon / 4)\n",
        "                    else:\n",
        "                        self.policy[i, j, action_index] = self.epsilon / 4\n",
        "                if np.argmax(self.policy[i, j]) != best_action:\n",
        "                    policy_stable = False\n",
        "        return policy_stable\n",
        "\n",
        "    def print_route(self, route):\n",
        "        print(\"Rota criada:\")\n",
        "        for r in range(self.size):\n",
        "            row = \"\"\n",
        "            for c in range(self.size):\n",
        "                if (r, c) in route:\n",
        "                    row += \" . \"\n",
        "                elif (r, c) in self.mountains:\n",
        "                    row += \" M \"\n",
        "                elif (r, c) in self.quicksand:\n",
        "                    row += \" Q \"\n",
        "                elif (r, c) == self.end:\n",
        "                    row += \" E \"\n",
        "                else:\n",
        "                    row += \" - \"\n",
        "            print(row)\n",
        "        print()\n",
        "\n",
        "    def simulate_optimal_policy(self):\n",
        "        state = (0, 0)\n",
        "        route = [state]\n",
        "        steps = 0\n",
        "        while not self.is_terminal(state) and state not in self.quicksand:\n",
        "            action = ['up', 'down', 'left', 'right'][np.argmax(self.policy[state])]\n",
        "            next_state, _ = self.get_next_state_and_reward(state, action)\n",
        "            route.append(next_state)\n",
        "            steps += 1\n",
        "            if next_state == state or next_state in self.quicksand:\n",
        "                break\n",
        "            state = next_state\n",
        "        return route, steps\n",
        "\n",
        "    def print_values(self):\n",
        "        print(\"Grid com valores esperados:\")\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                if (i, j) in self.mountains:\n",
        "                    print(\" M \", end=\"\")\n",
        "                elif (i, j) in self.quicksand:\n",
        "                    print(\" Q \", end=\"\")\n",
        "                elif (i, j) == self.end:\n",
        "                    print(\" E \", end=\"\")\n",
        "                else:\n",
        "                    print(f\"{self.state_values[i, j]:.2f}\", end=\" \")\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "gridworld = GridworldEpsilonGreedy()\n",
        "policy_stable = False\n",
        "while not policy_stable:\n",
        "    gridworld.evaluate_policy()\n",
        "    policy_stable = gridworld.improve_policy()\n",
        "\n",
        "gridworld.print_values()\n",
        "route, steps = gridworld.simulate_optimal_policy()\n",
        "gridworld.print_route(route)\n",
        "print(f\"Número de passos até o final: {steps}\")\n"
      ],
      "metadata": {
        "id": "C3d7kr9iD-MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cacc0b-d029-483f-b017-c9efd7b12d6f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid com valores esperados:\n",
            "-85.27 -84.82 -83.86 -82.43 -80.92 -79.53 -78.48 -77.92 \n",
            "-85.13 -84.71 -83.68 -81.81 -80.02 -78.36 -77.13 -76.46 \n",
            "-84.82 -84.58  M -80.39 -78.17 -75.90 -74.27 -73.39 \n",
            "-84.12 -83.60 -82.43  M -75.49 -71.82 -69.63 -68.37 \n",
            "-83.32 -82.58 -80.56 -75.88  M -65.14 -62.83 -60.81 \n",
            "-82.58 -82.15  Q -70.22 -63.36 -59.35 -54.23 -49.65 \n",
            "-81.56 -80.56 -78.50  Q -59.04 -53.02 -43.24 -31.88 \n",
            "-80.80 -79.26 -75.57 -67.97 -59.08 -48.57 -31.52  E \n",
            "\n",
            "Rota criada:\n",
            " .  .  .  .  .  .  -  - \n",
            " -  -  -  -  -  .  -  - \n",
            " -  -  M  -  -  .  -  - \n",
            " -  -  -  M  -  .  -  - \n",
            " -  -  -  -  M  .  -  - \n",
            " -  -  Q  -  -  .  -  - \n",
            " -  -  -  Q  -  .  .  - \n",
            " -  -  -  -  -  -  .  . \n",
            "\n",
            "Número de passos até o final: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndE7tecAfZTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qwG4Z6pfaoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tC0vc991gSdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzFg9RuJgUDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridworldPolicyIteration:\n",
        "    def __init__(self, size=8, discount_factor=0.99, theta=0.01):\n",
        "        self.size = size\n",
        "        self.discount_factor = discount_factor\n",
        "        self.theta = theta\n",
        "        self.values = np.zeros((size, size))\n",
        "        self.policy = np.zeros((size, size, 4)) + 0.25\n",
        "        self.mountains = [(7, 2), (1, 3), (4, 4)]\n",
        "        self.quicksand = [(5, 2), (6, 3)]\n",
        "        self.end = (7, 7)\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return state == self.end\n",
        "\n",
        "    def get_next_state_and_reward(self, state, action):\n",
        "        if self.is_terminal(state):\n",
        "            return state, 0\n",
        "        if state in self.quicksand:\n",
        "            return state, -100\n",
        "\n",
        "        r, c = state\n",
        "        if action == 'up':\n",
        "            next_r, next_c = max(r - 1, 0), c\n",
        "        elif action == 'down':\n",
        "            next_r, next_c = min(r + 1, self.size - 1), c\n",
        "        elif action == 'left':\n",
        "            next_r, next_c = r, max(c - 1, 0)\n",
        "        elif action == 'right':\n",
        "            next_r, next_c = r, min(c + 1, self.size - 1)\n",
        "\n",
        "        if (next_r, next_c) in self.mountains or (next_r, next_c) in self.quicksand:\n",
        "            next_r, next_c = r, c\n",
        "\n",
        "        reward = -100 if (next_r, next_c) in self.quicksand else -1\n",
        "\n",
        "        return (next_r, next_c), reward\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for r in range(self.size):\n",
        "                for c in range(self.size):\n",
        "                    if self.is_terminal((r, c)):\n",
        "                        continue\n",
        "                    v_old = self.values[r, c]\n",
        "                    v_new = 0\n",
        "                    for action, action_prob in zip(['up', 'down', 'left', 'right'], self.policy[r, c]):\n",
        "                        (next_r, next_c), reward = self.get_next_state_and_reward((r, c), action)\n",
        "                        v_new += action_prob * (reward + self.discount_factor * self.values[next_r, next_c])\n",
        "                    self.values[r, c] = v_new\n",
        "                    delta = max(delta, abs(v_old - v_new))\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        policy_stable = True\n",
        "        for r in range(self.size):\n",
        "            for c in range(self.size):\n",
        "                if self.is_terminal((r, c)):\n",
        "                    continue\n",
        "                chosen_action = np.argmax(self.policy[r, c])\n",
        "                action_values = []\n",
        "                for action in ['up', 'down', 'left', 'right']:\n",
        "                    (next_r, next_c), reward = self.get_next_state_and_reward((r, c), action)\n",
        "                    action_values.append(reward + self.discount_factor * self.values[next_r, next_c])\n",
        "                best_action = np.argmax(action_values)\n",
        "                if chosen_action != best_action:\n",
        "                    policy_stable = False\n",
        "                self.policy[r, c] = np.eye(4)[best_action]\n",
        "        return policy_stable\n",
        "\n",
        "    def print_values(self):\n",
        "        print(\"Grid com valores esperados:\")\n",
        "        for r in range(self.size):\n",
        "            for c in range(self.size):\n",
        "                if (r, c) in self.mountains:\n",
        "                    print(\" M \", end=\" \")\n",
        "                elif (r, c) in self.quicksand:\n",
        "                    print(\" Q \", end=\" \")\n",
        "                elif (r, c) == self.end:\n",
        "                    print(\" E \", end=\" \")\n",
        "                else:\n",
        "                    print(f\"{self.values[r, c]:5.2f}\", end=\" \")\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "    def simulate_optimal_policy(self):\n",
        "        state = (0, 0)\n",
        "        route = [state]\n",
        "        steps = 0\n",
        "        while not self.is_terminal(state) and state not in self.quicksand:\n",
        "            action = ['up', 'down', 'left', 'right'][np.argmax(self.policy[state])]\n",
        "            next_state, _ = self.get_next_state_and_reward(state, action)\n",
        "            route.append(next_state)\n",
        "            steps += 1\n",
        "            if next_state == state or self.is_terminal(next_state):\n",
        "                break\n",
        "            state = next_state\n",
        "        return route, steps\n",
        "\n",
        "    def print_route(self, route):\n",
        "        print(\"Rota criada:\")\n",
        "        for r in range(self.size):\n",
        "            row = \"\"\n",
        "            for c in range(self.size):\n",
        "                if (r, c) in route:\n",
        "                    row += \" . \"\n",
        "                elif (r, c) in self.mountains:\n",
        "                    row += \" M \"\n",
        "                elif (r, c) in self.quicksand:\n",
        "                    row += \" Q \"\n",
        "                elif (r, c) == self.end:\n",
        "                    row += \" E \"\n",
        "                else:\n",
        "                    row += \" - \"\n",
        "            print(row)\n",
        "        print()\n",
        "\n",
        "gridworld = GridworldPolicyIteration()\n",
        "policy_stable = False\n",
        "while not policy_stable:\n",
        "    gridworld.policy_evaluation()\n",
        "    policy_stable = gridworld.policy_improvement()\n",
        "\n",
        "gridworld.print_values()\n",
        "route, steps = gridworld.simulate_optimal_policy()\n",
        "gridworld.print_route(route)\n",
        "print(f\"Número de passos até o final: {steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKS3uhMR8QPs",
        "outputId": "8e6c922f-c20d-4443-d1ab-4ff92a570693"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid com valores esperados:\n",
            "-13.13 -12.25 -11.36 -10.47 -9.56 -8.65 -7.73 -6.79 \n",
            "-12.25 -11.36 -10.47  M  -8.65 -7.73 -6.79 -5.85 \n",
            "-11.36 -10.47 -9.56 -8.65 -7.73 -6.79 -5.85 -4.90 \n",
            "-10.47 -9.56 -8.65 -7.73 -6.79 -5.85 -4.90 -3.94 \n",
            "-9.56 -8.65 -7.73 -6.79  M  -4.90 -3.94 -2.97 \n",
            "-10.47 -9.56  Q  -5.85 -4.90 -3.94 -2.97 -1.99 \n",
            "-11.36 -10.47 -11.36  Q  -3.94 -2.97 -1.99 -1.00 \n",
            "-12.25 -11.36  M  -3.94 -2.97 -1.99 -1.00  E  \n",
            "\n",
            "Rota criada:\n",
            " .  -  -  -  -  -  -  - \n",
            " .  -  -  M  -  -  -  - \n",
            " .  -  -  -  -  -  -  - \n",
            " .  -  -  -  -  -  -  - \n",
            " .  .  .  .  M  -  -  - \n",
            " -  -  Q  .  .  -  -  - \n",
            " -  -  -  Q  .  -  -  - \n",
            " -  -  M  -  .  .  .  . \n",
            "\n",
            "Número de passos até o final: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "rRnidSsp_Abz",
        "outputId": "2fab0b5d-cebf-4dae-dc01-078555e76c4b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GridworldPolicyIteration' object has no attribute 'print_values'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-dadfda9b38f7>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Imprime os valores dos estados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mgridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Simula e imprime a rota e o número total de passos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridworldPolicyIteration' object has no attribute 'print_values'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhT5PpfGPeqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddZ-oBBpb-Ke"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}